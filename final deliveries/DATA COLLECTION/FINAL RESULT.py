# -*- coding: utf-8 -*-
"""Copy of Copy of Copy of IBM PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oIC2a9f1Z_AqQQU8LvTm9pc5G6AR9mrX
"""

#importing all necessary libraries for future analysis of the  dataset
import pandas as pd 
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
import math
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

#connecting to drive
from google.colab import drive
drive.mount('/content/drive')

#loading the dataset using read_csv function in pandas library
data=pd.read_csv('/content/drive/MyDrive/autos.csv',encoding="ISO-8859-1",low_memory=False)

#Reading the first five rows in csv file using head function in pandas library
data.head()

#Reading the last five rows in csv file using tail function
data.tail()

#checking the number of rows and columns in a dataset to understand the size that we are working with 
data.shape

#checking the size of the dataset 
data.size

#list of names of each column in a dataset
data.columns

#checking the every datatype of column in the dataset
data.info()

#knowing  the total number of unique values in each column using nunique function
data.nunique()

"""## ***HANDLING MISSING VALUES***"""

#after looking at the head of the dataset we have NaN and missing values
#To find the of missing values in each column 
#if present it shows true otherwise it shows false
data.isna().any()

#To find the count of missing values each column using sum function
data.isnull().sum()

#Finding the description of the dataset using describe function like mean,median etc.,
data.describe()

#Finding the mode of vehicleType column using mode function
data['vehicleType'].mode()

#total value_counts in vehicleType column
data['vehicleType'].value_counts()

#Replacing all NaN values in vehicleType column using mode 
data['vehicleType'].fillna("limousine",inplace=True)

#Finding the mode of vehicleType column using mode function
data['gearbox'].mode()

#Replacing all NaN values in gearbox column using mode 
data['gearbox'].fillna("manuell",inplace=True)

#Finding the mode of model column using mode function
data['model'].mode()

#Replacing all NaN values in model column using mode 
data['model'].fillna("golf",inplace=True)

#Finding the mode of fueltype column using mode function
data['fuelType'].mode()

#Replacing all NaN values in model column using mode 
data['fuelType'].fillna("benzin",inplace=True)

#Finding the mode of notRepairedDamage column using mode function
data['notRepairedDamage'].mode()

#Replacing all NaN values in notRepairedDamage column using mode 
data['notRepairedDamage'].fillna("nein",inplace=True)

data.head()

"""## ****OUTLIERS DETECTION AND REPLACING OUTLIERS****"""

sns.boxplot(data['price'])

#finding the interquartilerange of price column
q1=data['price'].quantile(0.25)
q3=data['price'].quantile(0.75)
iqr=q3-q1
lower_bound=q1-1.5*iqr
upper_bound=q3+1.5*iqr

#replacing the outliers of price column with mean
data['price']=np.where(data['price']>upper_bound,upper_bound,np.where(data['price']<lower_bound,upper_bound,data['price']))

#boxplot for price column
sns.boxplot(data['price'])

#finding the interquartilerange of kilometer column and replacing the outliers with mean 
q1=data['kilometer'].quantile(0.25)
q3=data['kilometer'].quantile(0.75)
iqr=q3-q1
lower_bound=q1-1.5*iqr
upper_bound=q3+1.5*iqr
data['kilometer']=np.where(data['kilometer']>upper_bound,data['kilometer'].mean(),np.where(data['kilometer']<lower_bound,data['kilometer'].mean(),data['kilometer']))

#boxplot for kilometer column
sns.boxplot(data['kilometer'])

#finding the interquartilerange of powerPS column and replacing the outliers with lower_bound,upper_bound
q1=data['powerPS'].quantile(0.25)
q3=data['powerPS'].quantile(0.75)
iqr=q3-q1
lower_bound=q1-1.5*iqr
upper_bound=q3+1.5*iqr
data['powerPS']=np.where(data['powerPS']>upper_bound,upper_bound,np.where(data['powerPS']<lower_bound,lower_bound,data['powerPS']))

#boxplot for powerPS column
sns.boxplot(data['powerPS'])

#finding the interquartilerange of yearOfRegistration column and replacing the outliers with mean 
q1=data['yearOfRegistration'].quantile(0.25)
q3=data['yearOfRegistration'].quantile(0.75)
iqr=q3-q1
lower_bound=q1-1.5*iqr
upper_bound=q3+1.5*iqr
data['yearOfRegistration']=np.where(data['yearOfRegistration']>upper_bound,data['yearOfRegistration'].mode(),np.where(data['yearOfRegistration']<lower_bound,data['yearOfRegistration'].mode(),data['yearOfRegistration']))

#boxplot for yearOfRegistration column
sns.boxplot(data['yearOfRegistration'])

#boxplot for monthOfRegistation column
sns.boxplot(data['monthOfRegistration'])

#Reading the first five rows of cleaned dataset using head function
data.head()

"""# ****EXPLORATORY DATA ANALYSIS****

### **Exploring Categorical Features**
"""

#list of all categorical columns
list(data.select_dtypes('object'))

data['seller'].value_counts()

#counting public and gewerblich types in seller column using countplot
sns.countplot(data['seller'],palette='coolwarm',saturation=0.9)

data['abtest'].value_counts()

#counting the percentage of different types in abtest column using pie chart
plt.pie(data['abtest'].value_counts(),startangle=90,labels=['test','control'],shadow=True,autopct='%1.2f%%')
plt.legend()
plt.title("abtest")

data['offerType'].value_counts()

#counting angebot and gesuch types in offerType column using countplot
sns.countplot(data['offerType'],palette='spring')

data['vehicleType'].value_counts()

#count of each type in vehicleType column
sns.countplot(data['vehicleType'])

#count of each type in gearbox column
sns.countplot(data['gearbox'],palette='pastel')

data['model'].value_counts()

#top 10 models in model column
plt.figure(figsize =(15,6))
sns.countplot(data['model'].value_counts().head(10))

data['fuelType'].value_counts()

plt.figure(figsize =(15,6))
sns.countplot(data['fuelType'])

data['brand'].value_counts().head()

#count of eaach brand in brand column
plt.figure(figsize =(10,6))
sns.countplot(data['brand'])

data['notRepairedDamage'].value_counts()

sns.countplot(data['notRepairedDamage'],palette='spring')

a=list(data.select_dtypes('number'))
for i in a:
    fig = plt.figure(figsize=(9, 6))
    ax = fig.gca()
    feature = data[i]
    feature.hist(bins=50, ax = ax)
    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)
    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)    
    ax.set_title(i)
plt.show()

#correlation of  dataset using correaltion function 
correlation=data.corr()
correlation

#exploring the correlation using heatmap 
plt.figure(figsize=(15,10))
sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')

"""## 1.SELLER VS PRICE """

plt.figure(figsize=(8,4))
sns.barplot(x='seller',y='price',data=data,palette='dark')

"""## 2.VEHICLETYPE VS PRICE"""

plt.figure(figsize=(8,6))
sns.barplot(x='vehicleType',y='price',data=data,ci=100,capsize=0.3,saturation=0.8)

"""## 3.MODEL  VS  PRICE"""

plt.figure(figsize=(25,5))
sns.barplot(x='model',y='price',data=data)

"""## 4.KILOMETER VS PRICE

"""

sns.kdeplot(x='kilometer',y='price',data=data,palette='husl')

"""## 5.BRAND VS PRICE"""

plt.figure(figsize=(25,5))
sns.barplot(x='brand',y='price',data=data)

"""# 6. YEAR OF REGISTRATION   VS   PRICE"""

plt.figure(figsize=(15,5))
sns.stripplot(x='yearOfRegistration',y='price',data=data)

"""# 7.FUEL TYPE   VS  PRICE"""

sns.barplot(x='fuelType',y='price',data=data)

"""# 8.GEARBOX VS KILOMETER"""

sns.pointplot(x='gearbox',y='kilometer',hue='fuelType',data=data,ci=99,saturation=0.8,capsize=0.3)

"""
# 9.KILOMETER VS PRICE"""

sns.scatterplot(x='fuelType',y='kilometer',data=data)

"""
## DISTRIBUTION PLOT """

#examing the distribution of price column using distplot in seaborn library
plt.figure(figsize=(15,5))
sns.distplot(data['price'])

parameters={'seller':{'privat':0,'gewerblich':1},
            'abtest':{'test':0,'control':1},
            'notRepairedDamage':{'nein':0,'ja':1},
            'vehicleType':{'limousine':0,'kleinwagen':1,'kombi':2,'bus':3,'cabrio':4,'coupe':5,'suv':6,'andere':7},
            'fuelType':{'benzin':0,'diesel':1,'lpg':2,'cng':3 ,'hybrid':4,'andere':5,'elektro':6}}
data_df=data.replace(parameters)
data_df.head()

#converting all catogorical columns into numerical columns using get_dummies function
Fe_df_cleaned=pd.get_dummies(data_df,columns=['offerType','gearbox'],drop_first=True)
Fe_df_cleaned.head()

#shape of the dataset after label encoding
Fe_df_cleaned.shape

Fe_df_cleaned.columns

#removing unncessary columns in the dataset
main_df=Fe_df_cleaned.drop(columns=['dateCrawled','dateCreated','name','lastSeen','brand','model'],axis=1)
main_df.head()

#multivariate analysis
plt.figure(figsize=(15,5))
sns.pairplot(data)

#dividing the dataset into dependent and independent feature
Independent=main_df.drop(['price'],axis=1)
Dependent=main_df['price']
Independent.head()

Dependent.head()

"""## STANDARDIZATION"""

#importing standardscalar from scikitlearn to standardize data values into standard format
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
scaled=pd.DataFrame(sc.fit_transform(Independent),columns=Independent.columns)

scaled.head()

"""## ***DIVIDING DATA INTO TRAIN AND TEST***"""

#divivng the dataset into train and test using train_test_split function
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(scaled,Dependent,test_size=0.25,random_state=0)

#shape of the training data
x_train.shape

#shape of the test data
x_test.shape

#Independent features of  training data after dividing training and testing
x_train.head()

#Dependent feature of tetsing data after dividing training and testing 
y_train.head()

"""## ***MODEL I : RANDOMFOREST REGRESSOR***"""

from sklearn.ensemble import RandomForestRegressor

#training the data to randomforestregression algorithm
rfr=RandomForestRegressor()
model=rfr.fit(x_train,y_train)

#predicting the test data
y_pred=model.predict(x_test)

"""## ***EVALUATION METRICS***"""

#importing necessary libraries to find evaluation of the model
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
import math

#mean squared error
MSE=mean_squared_error(y_test,y_pred)
print("MSE:",MSE)

#Root mean squared error
RMSE=math.sqrt(MSE)
print("RMSE:",RMSE)

#checking the performance of the model using r2_score 
r2=r2_score(y_test,y_pred)
print("R2_score:",r2)

#Adjusted R square
Adjusted_R2=1-(1-r2*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1)))
print("Adjusted R2:",Adjusted_R2)

"""After trying multiple regression algorithms like Linear regression,Lasso Regression,Ridge Regression ,Decision tree Regression and  RandomForestRegression. 
***Random Forest  Regression is giving highest accuracy.****
"""

#plot for predicted and actual price 
plt.figure(figsize=(10,5))
plt.plot(y_pred[0:20])
plt.plot(np.array(y_test[0:20]))
plt.legend(["predicted","actual"])
plt.show()

print("The accuracy of the RandomForestRegression:",r2)

import joblib

joblib.dump(model,'Car Resale Value Prediction')

gsk=joblib.load('Car Resale Value Prediction')

gsk.predict([['0','0','0','2011','0','1500','9','0','0','0','70464','1','1']])

gsk.predict([['0','1','0','2011','1','15000','9','0','1','0','70464','1','1']])

